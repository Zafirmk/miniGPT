# miniGPT

Language translation using an encoder-decoder transformer model trained using distributed data parallel on compute canada clusters. Sample training done on 16 Tesla V100-SXM2-32GB GPUs spread across 4 nodes.

## References
- [Attention is all you need](https://arxiv.org/pdf/1706.03762)
- [Creating a transformer from scratch by Benjamin Warner](https://benjaminwarner.dev/2023/07/01/attention-mechanism)
